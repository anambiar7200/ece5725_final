<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Starter Template for Bootstrap</title>

  <!-- Bootstrap core CSS -->
  <link href="dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="starter-template.css" rel="stylesheet">

  <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
  <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar"
          aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="#">Smart Lock</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li class="active"><a href="#">Home</a></li>
          <li><a href="#intro">Introduction</a></li>
          <li><a href="#obj">Project Objective</a></li>
          <li><a href="#design">Design and Testing</a></li>
          <li><a href="#result">Results and Conclusion</a></li>
          <li><a href="#future">Future Work</a></li>
          <li><a href="#budget">Budget and Parts</a></li>
          <li><a href="#team">Team Member Info</a></li>
          <li><a href="#references">References</a></li>
          <li><a href="https://github.com/anambiar7200/ece5725_final" class=button>Code Appendix</a></li>
        </ul>
      </div>
      <!--/.nav-collapse -->
    </div>
  </nav>

  <div class="container">

    <div class="starter-template">
      <h1>Smart Lock</h1>
      <p class="lead">ECE 5725 Final Project - Spring 2022<br>Alisha Kochar (ak2255), Anusha Nambiar (aan29)</p>
    </div>

    <hr>
    <div class="center-block">
      <iframe width="640" height="360" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0"
        allowfullscreen></iframe>
      <h4 style="text-align:center;">Demonstration Video</h4>
    </div>

    <hr id="intro">

    <div style="text-align:center;">
      <h2>Introduction</h2>
      <p style="text-align: left;padding: 0px 30px;">Our Smart Lock uses <strong>facial recognition</strong> to grant
        access to users recognized by the system. Upon access granted, all users are able to lock or unlock
        the system. The first method of authentication uses one of the piTFT buttons, which upon being pressed,
        verifies that there is a user associated with that button. If recognized by the system, the system will then use
        the piCamera to scan the user’s face
        and use <strong>facial recognition</strong> via <strong>machine learning</strong> to either grant access if the
        user is the correct individual associated
        with the piTFT button, or deny access if the user if not the correct individual associated with the piTFT
        button. Our
        smart lock has two modes –– 1) a superuser mode –– this individual has the ability to add new users,
        remove existing users, view the recent access history associated with their tag (date+time, access
        permissions (i.e. granted or denied)), as well as lock and unlock the lock, and 2) a regular user
        mode –– this individual has the ability to either lock or unlock the lock, as well as view the recent
        access history associated with their tag. The Smart Lock has been developed using a <strong>Raspberry Pi
          4B</strong> as
        the final project of the <strong>ECE 5725 Design with Embedded OS</strong> class at Cornell University.</p>
    </div>

    <hr id='obj'>

    <div class="row">
      <h2 style="text-align:center;">Project Objective</h2>
      <div class="col-md-4" style="text-align:center;">
        <img class="img-rounded" src="pics/initial_sketch.png" alt="Generic placeholder image" width="330" height="240">
      </div>
      <div class="col-md-8" style="font-size:18px;">
        <ul>
          <li>Permit users access to the system via an authentication process (1. recognizable piTFT button, 2. facial
            recognition via ML)</li>
          <li>Permit actions for users and recent history per each user mode (superuser vs. regular)</li>
        </ul>
      </div>
    </div>

    <hr id='design'>

    <div style="text-align:center;">
      <h2>Design and Testing</h2>
      <div class="row">
        <img src="pics/fsm.png" alt="Beginning project FSM">
      </div>
      <p style="text-align: left;padding: 0px 30px;">The development of our project was split into three stages: setting
        up the RFID tags and readers, setting up the facial recognition system, and setting up the lock interface. <br>
        Talk about RFID here <br> <br>
        The first step in developing the facial recognition code was setting up and getting comfortable with the
        PiCamera. Once we were able to take both single photos of ourselves and a series of photos from a stream, we
        began working with facial detection. <br><br>
        The facial detection code used a Haar cascade classifier (from the Python module <a
          href="https://github.com/opencv/opencv-python">openCV</a>) to detect faces in an
        image and then draw a rectangle around them. We did not end up using this code in the final project because we
        wanted to focus our display on the piTFT screen, but it helped us better understand the piCamera and openCV
        modules. <br><br>
        We first tried to build our facial recognition feature with the module <a
          href='https://viso.ai/computer-vision/deepface' />DeepFace</a>. This module is very powerful, and gives
        accurate predictions with only one training image. When we tested the code we wrote with this module on our
        laptops, it worked correctly, but when we tried to run this code on the RaspberryPi, it did not work. We
        discovered that this is because DeepFace uses TensorFlow, another commonly used Machine Learning module for
        Python. When we tried to install TensorFlow to resolve the dependency, we found that there is no stable version
        of TensorFlow compatible with Python 3.9 (the version we are using on our RaspberryPi). We attempted to
        downgrade to Python 3.8, but that caused other compatibility issues, so we decided not to use the DeepFace
        module. <br><br>
        After finding that DeepFace was not feasible, we switched to the module <a
          href="https://github.com/ageitgey/face_recognition">Face Recognition</a>. Our initial training code with this
        module
        read in all the
        users photos, stored on the device in folders titled "userN" (where N is the user number), detected and encoded
        the faces in each photo, and then stored these encodings as well as the associated user names in lists. The
        recognition code then compared the face encoding in a test image against all the other face endocings, and
        returned the name associated with the one most similar to the test image. <br><br>
        Initially, we had no way of storing these encodings, so we had to run the training every time before we could
        run an image comparision. This made our code incredibly slow, and not feasible for an embedded system. We
        thought about storing the data in a text file, but that made it
        difficult and unwieldy to access since the encodings are large matrices. We eventually decided to use the Python
        library <a href="https://docs.python.org/3/library/pickle.html">Pickle</a>. We use Pickle to serialize the data
        (convert it
        into byte streams) and then store this in a file on the RPi. We can then de-serialize the data quickly back into
        the original format. This allowed us to access the encodings without having to run through all the images and
        calculate them evey time. We also modified our get_encodings function so that we did not have to re-add existing
        users every time we wanted to train the data, but rather just add new users to the end of the file. <br> <br>
        Because our encodings and names and stored in separate files with coordinating indices, removing specific users
        from the system was a challenge. We tried reformatting the data as a dictionary and as a list of tuples, but
        found that these formats made it difficult to add new users and made the training less accurate. We decided to
        keep the original format of the data, but that meant that whenever we wanted to remove a user, we had to delete
        all the data, and then re-add the encodings for all the remaining users. For our system of only three users,
        this did not take too long (about 60 seconds), but in a larger system this would need to be modified. <br><br>
        We wanted our final project to avoid using the HDMI monitor, keyboard, and mouse, so that it would more closely
        resemble a real embedded system. To do this, we created various displays for the piTFT screen.


      </p>
    </div>

    <hr id='results'>

    <div style="text-align:center;">
      <h2>Results and Conclusion</h2>
      <p style="text-align: left;padding: 0px 30px;">
        We are satisfied with the end result of our project, though we were not able to accomplish one of our goals. Our
        original design
        integrated a more sophisticated two-factor authentication to grant users access to the system –– the first
        method of
        authentication would have involved a user-assigned RFID tag, which would be scanned by an RFID receiver. If
        recognized by the
        system, the system would then use the piCamera to scan the user’s face and use facial recognition to either
        grant access if the
        user is the correct individual associated with the tag, or deny access if the user if not the correct individ
        ual associated with
        the tag. At the beginning of working on this final project, the RFID receiver was the first task we worked on
        connecting to the
        RPi. It was originally successful, however, after approximately two weeks, our RFID receiver stopped reading our
        RFID tags. <br><br>

        We were able to move on from this unfortunate hurdle by switching to using the piTFT buttons as the first system
        “authentication”.
        This allowed us to move forward with our project and focus on the more involved second measure of authentication
        –– facial
        recognition –– as well as the user-specific actions upon being granted access to the smart lock system. <br><br>

        We are happy with the accuracy of our facial recognition system for the size of the project. The
        system is able to differentiate between users easily, and typically recognizes new photos of users as long as
        the lighting conditions are not too poor. The system is also quite efficient for its size. It takes about thirty
        seconds to add a new user and about a minute to remove one. <br><br>

        Using an incremental design process, as well as using GitHub to store our working files, we were able to
        complete this project at a good pace and by the deadline.
      </p>
    </div>
    <hr id='future'>

    <div style="text-align:center;">
      <h2>Future Work</h2>
      <p style="text-align: left;padding: 0px 30px;">
        Given more time, we definitely would have liked to experiment more with the RFID receiver. An idea we did not
        get to explore was
        working with a voltage amplifier to drive the RFID with 5V from the RPi, as opposed to the general 3.3V. <br>
        <br>
        Additionally, we wanted to integrate "promote user" and "demote user" functionality so that users could go from
        regular users to super users and vice versa.
        <br> <br>
        Finally, if we had more time we would modify our system to accomodate more users. Currently all the user
        training images, the encoding information, and the access history are all stored on the RaspberryPi. Since we
        did not have more than three users, we had enough storage, but for a realistic system with more users than that,
        we would need to integrate another data storage method, such as a database.
      </p>

    </div>

    <hr id='budget'>

    <div style="font-size:18px">
      <h2 style="text-align:center;">Budget and List of Parts</h2>

      <table>
        <tr>
          <th>Item</th>
          <!-- <th>Purchase Link</th> -->
          <th>Price</th>
          <th>Source</th>
        </tr>
        <tr>
          <td>Raspberry Pi 4B 2GB</td>
          <td>$35</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>PiTFT 2.8" Touchscreen</td>
          <td>$34.95</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Pi Cobbler + Breakout Cable</td>
          <td>$6.50</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Red and White RPi4 Case</td>
          <td>$5</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>SD Card 16GB</td>
          <td>$8.95</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Jumpers</td>
          <td>-</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Linear Actuator (50mm stroke, 50:1 ratio, 6 volts)</td>
          <td>$70.00</td>
          <td>Owned (Alisha)</td>
        </tr>
        <tr>
          <td>Patio Door Lock</td>
          <td>$23.65</td>
          <td>Owned (Alisha)</td>
        </tr>
        <tr>
          <td>ClickSent Text Message Service</td>
          <td>$20.00</td>
          <td>Purchased (Anusha)</td>
        </tr>
        <tr>
          <td style="text-align:right;"><strong>Total</strong></td>
          <td>$204.05</td>
          <td></td>
        </tr>
        <tr>
          <td style="text-align:right;"><strong>Actual Expenditure</strong></td>
          <td>$20</td>
          <td>Items bought for this project</td>
        </tr>

      </table>
    </div>
    <hr id='team'>
    <div style="text-align:center;">
      <h2>The Team</h2>
      <div class="col-md-6" style="font-size:16px">
        <img class="img-rounded" src="pics/Headshot_Kochar.png" alt="Alisha Headshot" width="240" height="240">
        <h4>Alisha Kochar</h4>
        <p><em>Electrical and Computer Engineering, MEng 2022</em></p>
        <p>ak2255@cornell.edu</p>
      </div>
      <div class="col-md-6" style="font-size:16px">
        <img class="img-rounded" src="pics/anusha_headshot_square.jpg" alt="Anusha Headshot" width="240" height="240">
        <h4>Anusha Nambiar</h4>
        <p><em>Electrical and Computer Engineering, 2022</em></p>
        <p>aan29@cornell.edu</p>
      </div>
      <p>We worked on this project collaboratively. Initially, Anusha worked on the Facial Recognition module, whereas
        Alisha
        implemented the RFID detection module and linear actuator functionality. Once the RFID module stopped working on
        our RPi,
        we worked collaboratively to explore other outlines for our project and moved into collectively designing the
        piTFT
        display as well as user functionality for our Smart Lock. We used GitHub to maintain the code base and updated
        each other
        frequently on new thoughts and ideas. We completed the documentation and website together as well.
      </p>
    </div>


    <hr id='references'>
    <div style="font-size:18px">
      <h2 style="text-align:center;">References</h2>
      <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
      <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
      <a href="http://getbootstrap.com/">Bootstrap</a><br>
      <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
      <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>

    </div>

    <!-- <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div> -->

  </div><!-- /.container -->




  <!-- Bootstrap core JavaScript
    ================================================== -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
  <script src="dist/js/bootstrap.min.js"></script>
  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
</body>

</html>