<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Starter Template for Bootstrap</title>

  <!-- Bootstrap core CSS -->
  <link href="dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="starter-template.css" rel="stylesheet">

  <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
  <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar"
          aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="#">Smart Lock</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li class="active"><a href="#">Home</a></li>
          <li><a href="#intro">Introduction</a></li>
          <li><a href="#obj">Project Objective</a></li>
          <li><a href="#design">Design and Testing</a></li>
          <li><a href="#result">Results and Conclusion</a></li>
          <li><a href="#future">Future Work</a></li>
          <li><a href="#budget">Budget and Parts</a></li>
          <li><a href="#team">Team Member Info</a></li>
          <li><a href="#references">References</a></li>
          <li><a href="https://github.com/anambiar7200/ece5725_final" class=button>Code Appendix</a></li>
        </ul>
      </div>
      <!--/.nav-collapse -->
    </div>
  </nav>

  <div class="container">

    <div class="starter-template">
      <h1>Smart Lock</h1>
      <p class="lead">ECE 5725 Final Project - Spring 2022<br>Alisha Kochar (ak2255), Anusha Nambiar (aan29)</p>
    </div>

    <hr>
    <div class="center-block">
      <iframe width="640" height="360" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0"
        allowfullscreen></iframe>
      <h4 style="text-align:center;">Demonstration Video</h4>
    </div>

    <hr id="intro">

    <div style="text-align:center;">
      <h2>Introduction</h2>
      <p style="text-align: left;padding: 0px 30px;">Our Smart Lock uses <strong>two-factor authentication</strong> to
        grant
        access to users recognized by the system. Upon access granted, all users are able to lock or unlock
        the system. The first method of authentication uses one of the piTFT buttons, which upon being pressed,
        verifies that there is a user associated with that button. If recognized by the system, the system will then use
        the piCamera to scan the user’s face
        and use <strong>facial recognition</strong> to either grant access if the
        user is the correct individual associated
        with the piTFT button, or deny access if the user if not the correct individual associated with the piTFT
        button –– our second measure of authentication. Our
        smart lock has two modes –– 1) a superuser mode –– this individual has the ability to add new users,
        remove existing users, view the recent access history associated with their tag (date+time, access
        permissions (i.e. granted or denied), as well as lock and unlock the lock and 2) a regular user
        mode –– this individual has the ability to either lock or unlock the lock, as well as view the recent
        access history associated with their tag. The Smart Lock has been developed using a <strong>Raspberry Pi
          4B</strong> as
        the final project of the <strong>ECE 5725 Design with Embedded OS</strong> class at Cornell University.</p>
    </div>

    <hr id='obj'>

    <div class="row">
      <h2 style="text-align:center;">Project Objective</h2>
      <div class="col-md-4" style="text-align:center;">
        <img class="img-rounded" src="pics/overview.png" alt="Project overview" width="330" height="240">
      </div>
      <div class="col-md-8" style="font-size:18px;">
        <ul>
          <li>Permit users access to the system via an authentication process (1. recognizable piTFT button, 2. facial
            recognition via ML)</li>
          <li>Permit actions for users and recent history per each user mode (superuser vs. regular)</li>
          <li>Send text alerts to superusers after denied access attempts</li>
          <li>Control bolt lock using an actuator</li>
        </ul>
      </div>
    </div>

    <hr id='design'>

    <div style="text-align:center;">
      <h2>Design and Testing</h2>
      <h4 style="text-align: left;">The following is the timeline we followed for our project:</h3>
        <ul style="text-align: left;">
          <li><strong>Week 1:</strong> initial scheduling/planning, set up basic RFID reading,
            set up camera
          </li>
          <li><strong>Week 2:</strong> code to write to history to RFID tags, linear actuator (lock hardware) setup,
            lock control logic
          </li>
          <li><strong>Week 3:</strong> facial recognition module, button logic
            (switched from RFIDs to piTFT buttons)</li>
          <li><strong>Week 4:</strong> piTFT display, code to add/remove users, store user hist from button press
            and display to piTFT
          </li>
          <li><strong>Week 5:</strong> project write-up (website) and final demo</li>
        </ul>

        <h4 style="text-align: left;"><strong>Week 1</strong></h4>
        <p style="text-align: left;padding: 0px 30px;">
          Our initial design project integrated an RFID as the first measure in user-authentication.
          We worked on setting up the RFID-readTag module which would read a tag scanned by a user, and
          could take one of two actions: <br> 1) allow the user to move to the second measure of user-authentication
          (i.e. the facial recognition module) <strong>if the tag is recognized by the system</strong> or <br> 2) depict
          an error message to the screen indicating that <strong>the tag is not recognized by the
            system</strong><br><br>
          Initially, we had some issues with integrating the RFID with the RPi –– this was caused by the piTFT and RPi
          sharing the
          same SPI. Thus, we needed to edit our /boot/config.txt file to register another SPI for our RPi. Once we
          editied this file, we were able to pair our RFID receiver to the RPi and use the
          <a
            href="https://github.com/ondryaso/pi-rc522?fbclid=IwAR1IXO7vXJcHdmke0vXWPXevCFbgDdIrM998u1YaQ0gzrX-hPBmRi9htXok">pirc522
            Python library </a>
          to develop our RFID module. We used the following hardware setup to connect the RFID receiver to the RPi:
          <br><br>
        </p>
        <table style="text-align: center; width: 50%;margin-left: auto;margin-right: auto;">
          <tr>
            <th>RFID</th>
            <th>Pi Pin</th>
            <th>Pin Name</th>
          </tr>
          <tr>
            <td>SDA</td>
            <td>37</td>
            <td>GPIO26</td>
          </tr>
          <tr>
            <td>SCK</td>
            <td>40</td>
            <td>SPI1 SCLK</td>
          </tr>
          <tr>
            <td>MOSI</td>
            <td>38</td>
            <td>SPI1 MOSI</td>
          </tr>
          <tr>
            <td>MISO</td>
            <td>35</td>
            <td>SPI1 MISO</td>
          </tr>
          <tr>
            <td>IRQ</td>
            <td>29</td>
            <td>GPIO5</td>
          </tr>
          <tr>
            <td>GND</td>
            <td>39</td>
            <td>Ground</td>
          </tr>
          <tr>
            <td>RST</td>
            <td>31</td>
            <td>GPIO6</td>
          </tr>
          <tr>
            <td>3.3V</td>
            <td>17</td>
            <td>VDD_3V3</td>
          </tr>

        </table><br>
        <p style="text-align: left;padding: 0px 30px;">
          Once our hardware and software setups were complete, we were able to successfully read the Unique IDentifiers
          (UID)
          for each tag waved near the RFID receiver.<br><br>
          We then moved onto integrating the piCamera with our RPi. We used the Raspberry Pi Camera V2 and connected it
          to the
          RPi via the ribbon connector on the camera into the connector slot on the RPi. Please refer to the following
          image for our setup: </p><br><br>

        <div>
          <img src="pics/picamera_setup.jpg" alt="Picamera + RPi connection" height="300">
        </div>
        <br> <br>
        <p style="text-align: left;padding: 0px 30px;">
        During this week we also wrote code to store access histories on the RFID tags.
        </p>


        <h4 style="text-align: left;"><strong>Week 2</strong></h4>
        <p style="text-align: left;padding: 0px 30px;">
          Connecting the linear actuator to our RPi was fairly simple and somewhat familiar due to working with the DC
          motors
          we used in Lab 3. The linear actuator we used required a +6V drive, so we connected it to the bench power
          supply. We also used the bench power supply to create a common ground between the RPi and the
          actuator.
          We then connected the white RC input signal wire from the linear actuator directly to the RPi and breadboard
          via pin number 36 (GPIO16).
          <br><br>
          We then developed a simple Python program (actuator.py) to test the values needed to retract and expand the
          linear actuator ––
          correlating to locking and unlocking the physical lock. <br><br>

          Once this was complete, we completed the hardware setup for the lock, which can be seen in the following
          image: <br>
          <br>
        <div>
          <img src="pics/locked.png" alt="Hardware lock setup" height="300">
        </div><br> <br>
        New paragraph here.
        </p>

        <h4 style="text-align: left;"><strong>Week 3</strong></h4>

        <p style="text-align: left;padding: 0px 30px;">
          <!-- The development of our project was split into three stages: setting
        up the RFID tags and readers, setting up the facial recognition system, and setting up the lock interface. <br>
        Talk about RFID here <br> <br> -->
          The first step in developing the facial recognition code was setting up and getting comfortable with the
          PiCamera. Once we were able to take both single photos of ourselves and a series of photos from a stream, we
          began working with facial detection. <br><br>
          The facial detection code used a Haar cascade classifier (from the Python module <a
            href="https://github.com/opencv/opencv-python">openCV</a>) to detect faces in an
          image and then draw a rectangle around them. We did not end up using this code in the final project because we
          wanted to focus our display on the piTFT screen, but it helped us better understand the piCamera and openCV
          modules. <br><br>
          We first tried to build our facial recognition feature with the module <a
            href='https://viso.ai/computer-vision/deepface' />DeepFace</a>. This module is very powerful, and gives
          accurate predictions with only one training image. When we tested the code we wrote with this module on our
          laptops, it worked correctly, but when we tried to run this code on the RaspberryPi, it did not work. We
          discovered that this is because DeepFace uses TensorFlow, another commonly used Machine Learning module for
          Python. When we tried to install TensorFlow to resolve the dependency, we found that there is no stable
          version
          of TensorFlow compatible with Python 3.9 (the version we are using on our RaspberryPi). We attempted to
          downgrade to Python 3.8, but that caused other compatibility issues, so we decided not to use the DeepFace
          module. <br><br>
          After finding that DeepFace was not feasible, we switched to the module <a
            href="https://github.com/ageitgey/face_recognition">Face Recognition</a>. Our initial training code with
          this
          module
          read in all the
          users photos, stored on the device in folders titled "userN" (where N is the user number), detected and
          encoded
          the faces in each photo, and then stored these encodings as well as the associated user names in lists. The
          recognition code then compared the face encoding in a test image against all the other face endocings, and
          returned the name associated with the one most similar to the test image. <br><br>
          Initially, we had no way of storing these encodings, so we had to run the training every time before we could
          run an image comparision. This made our code incredibly slow, and not feasible for an embedded system. We
          thought about storing the data in a text file, but that made it
          difficult and unwieldy to access since the encodings are large matrices. We eventually decided to use the
          Python
          library <a href="https://docs.python.org/3/library/pickle.html">Pickle</a>. We use Pickle to serialize the
          data
          (convert it
          into byte streams) and then store this in a file on the RPi. We can then de-serialize the data quickly back
          into
          the original format. This allowed us to access the encodings without having to run through all the images and
          calculate them evey time. We also modified our get_encodings function so that we did not have to re-add
          existing
          users every time we wanted to train the data, but rather just add new users to the end of the file. <br> <br>
          Because our encodings and names and stored in separate files with coordinating indices, removing specific
          users
          from the system was a challenge. We tried reformatting the data as a dictionary and as a list of tuples, but
          found that these formats made it difficult to add new users and made the training less accurate. We decided to
          keep the original format of the data, but that meant that whenever we wanted to remove a user, we had to
          delete
          all the data, and then re-add the encodings for all the remaining users. For our system of only three users,
          this did not take too long (about 60 seconds), but in a larger system this would need to be modified. <br><br>
          This was also around the same time our RFID receiver stopped working with our RPi. We are not necessarily sure
          what exactly happened, but we tried many things such as replacing hardware and reverting back to different
          kernels,
          though all of these changes were unsuccessful. This is when we altered out project from two-factor
          authentication to one-factor
          and utilized the piTFT buttons for user-association. This definitely limited the scope of our project (as we
          would only have
          as many users as we could get buttons, whereas an RFID-based smart lock could have as many users as a database
          could store).
          <br><br>
          However, this was the solution we went with –– using the piTFT buttons instead. This was obviously much more
          simple
          to program than the RFID module. In this way, storing the user history was just as simple as storing the wheel
          history
          as we did in Lab 3. <br>
        </p>

        <h4 style="text-align: left;"><strong>Week 4</strong></h4>
        <p style="text-align: left;padding: 0px 30px;">
          During Week 4, we really had to make a big push to our project since we had to rewrite parts of the program we
          had already begun working on
          with the RFID receiver. Thus, we quickly made changes to the way we were adding and removing users, as well as
          storing user access histories.

          <br><br>
          For adding users, we associated users with piTFT buttons. Thus when a specific piTFT button was pressed, the
          added user would be associated
          with that respective button, and the system's pickle files would be added with information about that user
          (i.e. image encodings, userid, and histroy).
          <br><br>
          For removing users, we followed the same methodoloy –– when a piTFT button was pressed, the user associated
          with that button would be removed, as well as their
          information in the encodings, userid, and history pickle files.
          <br><br>
          Finally, we stored data access + time and whether the access was granted or denied into the pickle file that
          only stored histories for respective users.

          <br><br>
          We wanted our final project to avoid using the HDMI monitor, keyboard, and mouse, so that it would more
          closely
          resemble a real embedded system. To do this, we created various displays for the piTFT screen. The
          following FSM depicts our guiding vision for the piTFT display:
        </p> <br>
        <div class="row">
          <img src="pics/fsm.png" alt="Beginning project FSM" height="600">
        </div><br>
        <p style="text-align: left;padding: 0px 30px;">
          The initial display would request the user to "Scan your card" (or more likely, press their respective
          button), and then
          wait for system verification via the facial recognition module:
        </p>
        <div class="side">
          <div>
            <img src="pics/scan_card.png" alt="Scan your card">
          </div><br>
          <div>
            <img src="pics/verifying_user.png" alt="Verifying user">
          </div>
        </div><br>

        <p style="text-align: left;padding: 0px 30px;">
          If the user was not verified (i.e. their image was not recognized by the system due to them not actually being
          the user
          or the environment/lighting wuality was too poor for the piCamera to discern the user), then an invalid error
          message would pop up
          on the screen and the user would need to try again. TALK ABOUT TEXT MESSAGES HERE.
        </p>
        <div class="side">
          <div>
            <img src="pics/scan_card.png" alt="Scan your card">
          </div><br>
          <div>
            <img src="pics/verifying_user.png" alt="Verifying user">
          </div>
        </div><br>

        <p style="text-align: left;padding: 0px 30px;">
          Once a user was verified, one of two menu options would display. These two menus differ with respect to the
          type of user
          that is accessing the system –– whether they are a regular user who can only lock, unlock, and view their
          access history
          or a superuser who has additional capability of adding and removing other users.
        </p>
        <div class="side">
          <div>
            <img src="pics/superuser_menu.png" alt="Superuser menu">
          </div><br>
          <div>
            <img src="pics/reg_user_menu.png" alt="Regular user menu">
          </div>
        </div><br>



        <p style="text-align: left;padding: 0px 30px;">
          Upon selecting to lock the lock or unlock the lock, the linear actuator will retract or expand accordingly,
          and rest in the
          appropriate position.
        </p>
        <div class="side">
          <div class="row">
            <img src="pics/locked.png" alt="Unlocked lock">
          </div><br>
          <div class="row">
            <img src="pics/unlocked.png" alt="Locked lock">
          </div>
        </div><br>

        <p style="text-align: left;padding: 0px 30px;">
          Users can also view their three most recent access attempts to the system, which stores the date and time of
          access
          as well as if the access was granted or not.
        </p>
        <div class="row">
          <img src="pics/superuser_hist.png" alt="History">
        </div><br>

        <p style="text-align: left;padding: 0px 30px;">
          Superusers have the ability to add other users to the system. This is a two-step process in which the button
          they want
          associated with the user must be pressed, the user must then stand in front of the camera to allow for their
          picture to be
          taken for use by the facial recognition module. If the facial recognition module was able to verify the user
          in the images, the
          user will be added. Otherwise, if the user's face was not discernible in the pictures that were taken,
          the user will not be added, and must try again.
        </p>
        <!-- <div class="row">
    <img src="pics/added.png" alt="User added">
  </div> -->
        <div class="row">
          <img src="pics/stand_camera.png" alt="User told to stand in front of camera">
        </div><br>

        <div class="side">
          <div>
            <img src="pics/3.png" alt="Countdown 3">
          </div><br>
          <div>
            <img src="pics/3_2.png" alt="Countdown 32">
          </div><br>
          <div>
            <img src="pics/3_2_1.png" alt="Countdown 321">
          </div>
        </div><br>

        <div class="side">
          <div class="row">
            <img src="pics/standby.png" alt="Standby as facial encodings ensue">
          </div><br>
          <div class="row">
            <img src="pics/added.png" alt="User added">
          </div>
        </div><br>


        <p style="text-align: left;padding: 0px 30px;">
          Superusers also have the ability to remove other users from the system. In order to do this, the supermust
          must simply press
          the button associated with the user they would like to remove.
        </p>
        <!-- <div class="row">
      <img src="pics/added.png" alt="User added">
    </div> -->
        <div class="side">
          <div>
            <img src="pics/sel_del.png" alt="Selecting user to delete">
          </div><br>
          <div class="row">
            <img src="pics/standby.png" alt="Standby as user gets removed">
          </div>
        </div><br>

        <p style="text-align: left;padding: 0px 30px;">
          Thus, once a user gets removed and their respective button is no longer associated with anyone,
          they can no longer enter the system, and instead see a "user does not exist screen", as shown in the following
          image.
        </p>
        <div class="row">
          <img src="pics/user_dne.png" alt="User does not exist">
        </div><br>

        <p style="text-align: left;padding: 0px 30px;">
          Each menu display also had an EXIT button so users could exit the menu screen once they were done
          with their required actions. This EXIT took users back to the initial "Scan your card" screen so that
          another user could then access the system.
        </p>
    </div>

    <h4 style="text-align: left;"><strong>Week 5</strong></h4>
    <p style="text-align: left;padding: 0px 30px;">
      This final week was devoted to working on the project write-up (website) and final demo.
      We made a demo setup using a piece of wood we laser cut to fit the RPi piTFT screen+buttons through as well as
      the piCamera. We also drilled our lock setup into the board. This aimed to create an environment similar to that
      of
      an actual door lock mechanism on a wall+door. The following is our final demo setup:
      <div class="row">
        <img src="pics/demo_setup.png" alt="Final Demo Setup" style="width: 400px;margin: auto; display: block;">
      </div>
    </p>
    

    <hr id='results'>

    <div style="text-align:center;">
      <h2>Results and Conclusion</h2>
      <p style="text-align: left;padding: 0px 30px;">
        We are satisfied with the end result of our project, though we were not able to accomplish one of our goals. Our
        original design
        integrated a more sophisticated two-factor authentication to grant users access to the system –– the first
        method of
        authentication would have involved a user-assigned RFID tag, which would be scanned by an RFID receiver. If
        recognized by the
        system, the system would then use the piCamera to scan the user’s face and use facial recognition to either
        grant access if the
        user is the correct individual associated with the tag, or deny access if the user if not the correct individ
        ual associated with
        the tag. At the beginning of working on this final project, the RFID receiver was the first task we worked on
        connecting to the
        RPi. It was originally successful, however, after approximately two weeks, our RFID receiver stopped reading our
        RFID tags. <br><br>

        We were able to move on from this unfortunate hurdle by switching to using the piTFT buttons as the first system
        “authentication”.
        This allowed us to move forward with our project and focus on the more involved second measure of authentication
        –– facial
        recognition –– as well as the user-specific actions upon being granted access to the smart lock system. <br><br>

        We are happy with the accuracy of our facial recognition system for the size of the project. The
        system is able to differentiate between users easily, and typically recognizes new photos of users as long as
        the lighting conditions are not too poor. The system is also quite efficient for its size. It takes about thirty
        seconds to add a new user and about a minute to remove one. <br><br>

        Using an incremental design process, as well as using GitHub to store our working files, we were able to
        complete this project at a good pace and by the deadline.
      </p>
    </div>
    <hr id='future'>

    <div style="text-align:center;">
      <h2>Future Work</h2>
      <p style="text-align: left;padding: 0px 30px;">
        Given more time, we definitely would have liked to experiment more with the RFID receiver. An idea we did not
        get to explore was
        working with a voltage amplifier to drive the RFID with 5V from the RPi, as opposed to the general 3.3V. <br>
        <br>
        Additionally, we wanted to integrate "promote user" and "demote user" functionality so that users could go from
        regular users to super users and vice versa.
        <br> <br>
        Finally, if we had more time we would modify our system to accomodate more users. Currently all the user
        training images, the encoding information, and the access history are all stored on the RaspberryPi. Since we
        did not have more than three users, we had enough storage, but for a realistic system with more users than that,
        we would need to integrate another data storage method, such as a database.
      </p>

    </div>

    <hr id='budget'>

    <div style="font-size:18px">
      <h2 style="text-align:center;">Budget and List of Parts</h2>

      <table>
        <tr>
          <th>Item</th>
          <!-- <th>Purchase Link</th> -->
          <th>Price</th>
          <th>Source</th>
        </tr>
        <tr>
          <td>Raspberry Pi 4B 2GB</td>
          <td>$35</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>PiTFT 2.8" Touchscreen</td>
          <td>$34.95</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Pi Cobbler + Breakout Cable</td>
          <td>$6.50</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Red and White RPi4 Case</td>
          <td>$5</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>SD Card 16GB</td>
          <td>$8.95</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Jumpers</td>
          <td>-</td>
          <td>Lab</td>
        </tr>
        <tr>
          <td>Linear Actuator (50mm stroke, 50:1 ratio, 6 volts)</td>
          <td>$70.00</td>
          <td>Owned (Alisha)</td>
        </tr>
        <tr>
          <td>Patio Door Lock</td>
          <td>$23.65</td>
          <td>Owned (Alisha)</td>
        </tr>
        <tr>
          <td>ClickSent Text Message Service</td>
          <td>$20.00</td>
          <td>Purchased (Anusha)</td>
        </tr>
        <tr>
          <td style="text-align:right;"><strong>Total</strong></td>
          <td>$204.05</td>
          <td></td>
        </tr>
        <tr>
          <td style="text-align:right;"><strong>Actual Expenditure</strong></td>
          <td>$20</td>
          <td>Items bought for this project</td>
        </tr>

      </table>
    </div>
    <hr id='team'>
    <div style="text-align:center;">
      <h2>The Team</h2>
      <div class="col-md-6" style="font-size:16px">
        <img class="img-rounded" src="pics/Headshot_Kochar.png" alt="Alisha Headshot" width="240" height="240">
        <h4>Alisha Kochar</h4>
        <p><em>Electrical and Computer Engineering, MEng 2022</em></p>
        <p>ak2255@cornell.edu</p>
      </div>
      <div class="col-md-6" style="font-size:16px">
        <img class="img-rounded" src="pics/anusha_headshot_square.jpg" alt="Anusha Headshot" width="240" height="240">
        <h4>Anusha Nambiar</h4>
        <p><em>Electrical and Computer Engineering, 2022</em></p>
        <p>aan29@cornell.edu</p>
      </div>
      <p>We worked on this project collaboratively. Initially, Anusha worked on the Facial Recognition module, whereas
        Alisha
        implemented the RFID detection module and linear actuator functionality. Once the RFID module stopped working on
        our RPi,
        we worked collaboratively to explore other outlines for our project and moved into collectively designing the
        piTFT
        display as well as user functionality for our Smart Lock. We used GitHub to maintain the code base and updated
        each other
        frequently on new thoughts and ideas. We completed the documentation and website together as well.
      </p>
    </div>


    <hr id='references'>
    <div style="font-size:18px;text-align:center;">
      <h2>References</h2>
      <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
      <a href="http://getbootstrap.com/">Bootstrap</a><br>
      <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
      <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>
      <a href="https://github.com/opencv/opencv-python">openCV</a> <br>
      <a href="https://github.com/ageitgey/face_recognition">Face Recognition</a> <br>
      <a
        href="https://github.com/ondryaso/pi-rc522?fbclid=IwAR1IXO7vXJcHdmke0vXWPXevCFbgDdIrM998u1YaQ0gzrX-hPBmRi9htXok">PIRC522</a>
      <br>

    </div>

    <!-- <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div> -->

  </div><!-- /.container -->




  <!-- Bootstrap core JavaScript
    ================================================== -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
  <script src="dist/js/bootstrap.min.js"></script>
  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
</body>

</html>